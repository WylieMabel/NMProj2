---
editor_options: 
  markdown: 
    wrap: 72
---

# Network Modelling Project 2

**Matej Mrazek, Gabriele Spisani, Nicola Taddei, Mabel Wylie**

### Loading Data and Importing Packages

```{r}
install.packages('reticulate')
library(reticulate)
```

### Task 1:

#### 1)

This assumption is a good approximation for networks where agents make
decisions based solely on the current state of the network, disregarding
past states, and where the network's dynamics are either continuous or
sufficiently slow relative to the discrete-time intervals. This allows a
discrete-time system to be approximated by a continuous-time one.

For example, consider a network representing friendships in a high
school class and the academic performance of each student. While the
system may be naturally modeled as a discrete-time system—since students
primarily interact in the classroom for specific hours each day—the
gradual and slow changes in academic performance over a daily time scale
make it reasonable to approximate this system as a Markovian
continuous-time one.

In contrast, this assumption breaks down in networks such as business
relationships among companies and the quality of their manufactured
products. In the manufacturing industry, reputation often plays a
significant role, meaning that relationships between companies are
influenced by the past values of their attributes. As a result, the
Markovian assumption is violated in this context.

#### 2)

This assumption models agents as conscious and fully aware of the entire
network, as they are presumed to make decisions based on its structure
and actor-level attributes.

The high school class and academic performance network is a fitting
example where this assumption holds. In such a setting, students
consciously choose their friendships according to network structure and
agent attribues.

On the other hand, in a network representing infectious contacts between
individuals and super-spreading phenomena (where an actor-level
attribute models the conditional probability of infecting another
individual given a contact), this assumption breaks down. In such cases,
actors often have involuntary, indirect contacts with others—for
example, through shared surfaces in public transportation—thereby
violating the assumption.

#### 3)

This assumption is introduced to simplify the probability distribution
governing changes in the network structure or attributes. It is a
reasonable approximation in scenarios where changes occur gradually over
time and are relatively slow.

For instance, consider a network where agents are companies, represented
by a numerical value indicating their reputation, and a tie between two
agents A and B signifies that company A is purchasing a service from
company BB. In this case, events can be assumed to occur infrequently
enough that they happen one at a time.

However, if we also consider ties representing both the acquisition and
the sale of a service, network changes will involve two ties
simultaneously, as each transaction entails one company buying and
another selling. This would violate the assumption of single-event
changes.

#### 4)

This assumption is necessary for agents to make decisions that optimize
a network-level objective function.

For example, in the infectious contacts network discussed earlier,
actors are unaware of the actor-level attributes and therefore cannot
consciously make decisions to optimize a function of these attributes.

In contrast, in the high school class example, the network is small
enough for everyone to be aware of each other's academic performance.
This awareness enables students to form friendships based on this
attribute, thereby allowing decisions that align with a network-level
objective.

### Task 2: Network Evaluation Function

#### 1)

Using the RSiena manual, we write down the formulas for each effect as
follows:

-   Out-degree $s_{1i}(x) = \sum_j x_{ij}$

-   Reciprocity $s_{2i}(x) = \sum_j x_{ij} x_{ji}$

-   Transitive reciprocated triplets
    $s_{3i}(x) = \sum_{j, h} x_{ij} x_{ji} x_{ih} x_{hj}$

-   Indegree popularity
    $s_{4i}(x) = \sum_{j, k, j \neq k} x_{ji} x_{ki}$

-   Same covariate $s_{5i}(x, v) = \sum_j x_{ij} \mathbb{I}[v_i = v_j]$

Where $\mathbb{I}$ denotes the indicator function.

#### 2)

We considered it the easiest to write a quick python script that
implements each of the effects defined above. We can then run it on the
provided graph to obtain complete results (both $f_j$ and $p_j$ values
for every possible edge addition), which then makes it easy to answer
the provided questions. We share the python script below:

```{python python.reticulate = FALSE}
import scipy


# All graph vertices
task_V = {
    'a': 'white',
    'b': 'white',
    'c': 'gray',
    'd': 'gray', 
}

# All directed graph edges
task_E  = [
    ('a', 'b'),
    ('a', 'd'),
    
    ('b', 'c'),
    
    ('c', 'a'),
    
    ('d', 'a'),
    ('d', 'b')
]


# equal to x_{ij} for the given edges
def x(E, i, j):
    return 1 if (i, j) in E else 0

# equal to v_i for the given vertex
def v(V, i):
    return V[i]

# indicator variable
def I(a):
    return 1 if a else 0


# out-degree
def s1i(V, E, i):
    return sum([x(E, i, j) for j in V.keys()])

# reciprocity
def s2i(V, E, i):
    return sum([x(E, i, j) * x(E, j, i) for j in V.keys()])

# transitive reciprocated triplets
def s3i(V, E, i):
    return sum([x(E, i, j) * x(E, j, i) * x(E, i, h) * x(E, h, j) for j in V.keys() for h in V.keys()])

# indegree popularity
def s4i(V, E, i):
    return sum([x(E, j, i) * x(E, k, i) for j in V.keys() for k in V.keys() if j != k])

# same covariate
def s5i(V, E, i):
    return sum([x(E, i, j) * I(V[i] == V[j]) for j in V.keys()])

# the total score of the given vertices and edges
def score(V, E):
    s1 = sum([s1i(V, E, i) for i in V.keys()])
    s2 = sum([s2i(V, E, i) for i in V.keys()])
    s3 = sum([s3i(V, E, i) for i in V.keys()])
    s4 = sum([s4i(V, E, i) for i in V.keys()])
    s5 = sum([s5i(V, E, i) for i in V.keys()])
    
    # compute total score using the provided beta values
    return -1.2 * s1 + 1.5 * s2 + 1 * s3 + 0.5 * s4 + 1.3 * s5
    
# print the probabilities for all operations for a selected actor i
def step(V, E, i):
    results = []
    # base score (no changes)
    base = score(V, E)
    # go over all possible edges to add / remove
    for j in V.keys():
        if i == j: # source = target -> do nothing action
            results.append(("Do nothing", 0))
        elif not x(E, i, j): # Edge not present in graph - we are adding it
            results.append((f"Adding {i} -> {j}", score(V, (E + [(i, j)])) - base))
        else: # Edge present in graph - we are removing it
            results.append((f"Removing {i} -> {j}", score(V, [e for e in E if e != (i, j)]) - base))
    ops, scores = zip(*results)

    # convert computed scores to probabilities for each action
    next_probs = scipy.special.softmax(scores)

    # print results
    print ("Next step probabilities:" + "".join([f"\n\t* {op}: f={score:.1f}, p={v:.3f}" for op, score, v in zip(ops, scores, next_probs)]))
    
for i in task_V:
    step(task_V, task_E, i)
```

The formatted output is:\
Next step probabilities:

-    Do nothing: $f=0.0$, $p=0.022$

-   Removing $a \rightarrow b$: $f=-1.1$, $p=0.007$

-   Adding $a \rightarrow c$: $f=3.8$, $p=0.968$

-   Removing $a \rightarrow d$ : $f=-1.8$, $p=0.004$

Next step probabilities:

-   Adding $b \rightarrow a$ : $f=8.1$, $p=0.963$

-   Do nothing: $f=0.0$, $p=0.000$

-   Removing $b \rightarrow c$ : $f=1.2$, $p=0.001$

-   Adding $b \rightarrow d$ : $f=4.8$, $p=0.036$

Next step probabilities:

-   Removing $c \rightarrow a$ : $f=0.2$, $p=0.010$

-   Adding $c \rightarrow b$: $f=4.8$, $p=0.959$

-   Do nothing: $f=0.0$, $p=0.008$

-   Adding $c \rightarrow d$: $f=1.1$, $p=0.024$

Next step probabilities:

-   Removing $d \rightarrow a$: $f=-2.8$, $p=0.006$

-   Removing $d \rightarrow b$: $f=0.2$, $p=0.117$

-   Adding $d \rightarrow c$: $f=2.1$, $p=0.782$

-   Do nothing: $f=0.0$, $p=0.096$

We can now easily answer all the subtasks (assuming that each actor was
already selected for that particular ministep. To obtain the global
value - both sampling the actor and selecting this option - divide
percentage values by 4 to account for actor sampling.):

-   Probability that actor c adds a tie to b: $p=95.9\%$

-   Probability that actor b adds a tie to a: $p=96.3\%$

-   Probability that actor a deletes the tie to b: $p=0.7\%$

-   Probability that actor d does not change anything: $p=9.6\%$

### Task 3:

#### 1)
Implement the missing code so that the function simulation can be used to simulate the network evolution. Document the code. The al- gorithm is described in the file Simulating from SAOM available in the Lecture notes and additional material section on Moodle. Unconditional simulation is used.

```{r}
#' Simulate the network evolution between two time points
#'
#' @param n number of actors in the network
#' @param x1 network at time t1
#' @param W matrix of dyadic covariate
#' @param lambda rate parameter
#' @param beta1 outdegree parameter
#' @param beta2 reciprocity parameter
#' @param beta3 dyadic covariate parameter
#'
#' @return network at time t2
#'
#' @examples
#' netT1 <- matrix(c(
#'   0, 1, 0, 0, 0,
#'   0, 0, 0, 1, 0,
#'   0, 0, 0, 1, 1,
#'   1, 0, 1, 0, 0,
#'   0, 1, 1, 0, 1
#'   ), 
#'   nrow = 5, ncol =  5, byrow = TRUE)
#' W <- matrix(0, nrow = 5, ncol = 5)
#' values <- runif(5 * 2, 0, 1)
#' W[upper.tri(W)] <- values
#' W <- W + t(W)
#' netT2 <- simulation(5, netT1, W, 4, -2, 0.5, 1)
simulation <- function(n, x1, W, lambda, beta1, beta2, beta3) {
  t <- 0 # time
  x <- x1
  while (t < 1) {
    # Time until next change
    dt <- rexp(1, n * lambda)
    # Sample actor
    i <- sample(1:n,1)
    # Create an empty vector to hold the probabilities of actor i toggling a tie with each other actor
    probs <- rep(0, times = n)
    # For each actor p, we toggle the tie and compute the probability of this toggling using parameters
    for (p in 1:n) {
      # Copy current network
      xtemp <- x
      # Ties it can toggle (self-tie = do nothing)
      if (i!=p) {
        xtemp[i,p] <- 1 - xtemp[i,p]
      }
      probs[p] <- beta1 * sum(xtemp[i]) + beta2 * xtemp[p,i] + beta3 * W[i,p]
    }
    # Exponentiates individual probabilities then divides by the sum of over all
    probs <- exp(probs)
    probs <- probs / sum(probs)
    # Samples an actor j using the probabilities we calculated
    j <- sample(1:n, 1, prob=probs)
    # Toggles tie between i to j (self-tie = do nothing)
    if (i!=j) {
      x[i,j] <- 1 - x[i,j]
    }
    # Increment time step
    t <- t + dt
  }
  return(x)
}
```

#### 2)
Consider the two adjacency matrices in the files net1.csv and net2.csv. They are observations of two networks collected on a set of 22 actors at time t1 and t2, respectively. Additionally, the dyadic covariate is given in the file W.csv. Estimate the parameters of the SAOM with outdegree, reciprocity and dyadic covariate effects statistics using the function siena07.

```{r}
library(RSiena)

# read network and covariate csv
net1 <- as.matrix(read.csv("net1.csv", header = FALSE))
net2 <- as.matrix(read.csv("net2.csv", header = FALSE))
W <- as.matrix(read.csv("W.csv", header=FALSE))

# Creation of a Siena network object: dependent variable
SAOM <- sienaDependent(
  array(c(net1, net2), dim = c(22, 22, 2))
)

# Adjacency matrices and dyadic covariate stats are combined
# to create a siena data object by the function sienaDataCreate
wCoVar <- coDyadCovar(W)
mydata <- sienaDataCreate(SAOM,wCoVar)

# Gets effects to test for in model, reciprocity and outdegree are already included in the base effects
myeff <- getEffects(mydata)
myeff <- includeEffects(myeff, X, inclX, interaction1 = "wCoVar")

myAlgorithm <- sienaAlgorithmCreate(
  projname = "task3_2",
  nsub = 4, n3 = 3000, seed = 1908
)

# Estimate the model
model0 <- siena07(
  myAlgorithm,
  data = mydata, effects = myeff,
  returnDeps = TRUE,
  useCluster = TRUE, nbrNodes = 4, batch = FALSE
)

# Extract rate and theta estimates
lambda <- model0$rate
beta1 <- model0$theta[1]
beta2 <- model0$theta[2]
beta3 <- model0$theta[3]
```

#### 3)
Conditioning on the first observation, generate 1,000 simulations of the network evolution using the function simulation developed in (1) and setting the parameters with the results of the model estimated in (2).
Compute the triad census counts for each simulated network. Save the results in an R object1, named triadCensus, in which rows are the index of a simulated network and columns are the type of triads.

```{r}
library(sna)

# Initialises an empty list to store triad census counts for each simulation
datalist = list()

# Simulates a network given obtained parameters starting from the first observation network 1000 times
# Stores the triad census counts in a list of results
for (i in 1:1000) {
  # Starts from an empty matrix
  simulationI <- simulation(22, net1, W, lambda, beta1, beta2, beta3)
  triadCountI <- triad.census(simulationI, mode='digraph') 
  datalist[[i]] <- triadCountI
}

# Creates a dataframe out of the list of triad counts
triadDF <- do.call(rbind, datalist)
```

#### 4)
Use the simulated values of the triad census counts to evaluate the model’s goodness of fit. The second part of the code was written to this aim; complete the missing pieces of code to produce the violin plots. Additionally, write the code to compute the Mahalanobis distance and the p-value used in RSiena to assess the fit of the model with respect to the triad census auxiliary statistic. Remember to drop statistics with variance of 0 for the plot and Mahalanobis distance computation; report which statistics suffer this issue. The code should compute the following quantities:

##### i)
Standardize the simulated network statistics,i.e.,centered and scaled values of each type of triad given in the triadCensus object. Named the resulting object as triadCensusStd.

```{r}
meanx <- apply(triadDF,2,mean)
stdx <- apply(triadDF,2,sd)

zeroMean <- sweep(triadDF, 2, meanx, `-`)
triadCensusStd <- sweep(zeroMean, 2, stdx, `/`)
```

##### ii)
The variance-covariance matrix of the standarized simulated net- work statistics Pˆ and its generalized inverse.

```{r}
varCovMatrix <- cov(triadCensusStd)
genInv <- MASS::ginv(varCovMatrix)
```

##### iii)
Standardize the observed values of the triad census counts in the second observation with x ̄ and σx as in i).

```{r}
triadObs <- triad.census(net2,mode='digraph')
zeroMeanObs <- sweep(triadObs, 2, meanx, `-`)
triadCensusStdObs <- sweep(zeroMeanObs, 2, stdx, `/`)
```

##### iv)
Compute the Mahalanobis distance for each simulated and the ob- served network using the standardized values (computed on i) and iii)).

```{r}

#' Compute the Mahalanobis distance
#'
#' @param auxStats numerical vector with the mean centered or standardized
#'   auxiliar statistics
#' @param invCov numerical matrix with the inverse of the variance-covariance
#'   matrix of the auxiliar statistics in the simulated networks
#'
#' @return numeric value with the Mahalanobis distance of auxiliar stats
#'
#' @examples
#' mhd(c(2, 4) - c(1.5, 2), solve(matrix(c(1, 0.8, 0.8, 1), ncol = 2)))
mhd <- function(auxStats, invCov) {
  t(auxStats) %*% invCov %*% auxStats
}

# Wrapper function to apply mhd to a row of data
mhdWrapper <- function(auxStats) {
  mhd(auxStats,genInv)
}


mhdSim <- apply(triadCensusStd,1, mhdWrapper)

mhdObs <- apply(triadCensusStdObs, 1, mhdWrapper)
```

##### v)
Compute the percentage of simulated networks with Mahalanobis distance equal or greater than the observed network Mahalanobis distance.

```{r}
distGreater <- ifelse(mhdSim >= mhdObs, 1, 0)
pval <- sum(distGreater) / length(distGreater)
pval
```

Run the complete code to obtain the violin plots and the test on the Mahalanobis distance. Would you think that the model has a good fit based on the triad census auxiliary statistics and the p-value compute in (4)? Justify your answer.

```{r}
# install.packages(c("tidyverse", "ggplot2"))  # # run this line to install 
library(tidyverse)  # used: dplyr and tidyr
library(ggplot2)

# Given the array triadCensusStd, create a data frame from it in a long format, 
# do the same for the observed network statistics at time t2.
# Named the data frame "triadCensusDf" and "triadCensusObs".
# Drops statistics with variance of 0 for the plot.

triadCensusDf <- data.frame(triadCensusStd) |>
  select(where(~ var(.) > 0)) |>  # Drop statistics with zero variance
  pivot_longer(
    everything(),
    names_to = "triad", names_pattern = "^X(.+)$",
    values_to = "nnodes"
  )

# Compute the statistics of the observed network at time t2,
#  standardized using the stats from 2.4 literal i.
triadCensusObs <- data.frame(triadCensusStdObs) |> 
  data.frame() |>
  pivot_longer(
    everything(),
    names_to = "triad", names_pattern = "^X(.+)$",
    values_to = "nnodes"
  ) |>
  filter(triad %in% unique(triadCensusDf$triad))

# The following code computes the 5% and the 95% quantiles
# of the triad counts by type
percTriad <- triadCensusDf |>
  group_by(triad) |>
  summarise(
    quant05 = quantile(nnodes, prob = 0.05),
    quant95 = quantile(nnodes, prob = 0.95)
  ) |>
  pivot_longer(
    starts_with("quant"),
    names_to = "quant", names_pattern = "quant(.+)",
    values_to = "nnodes"
  )


# The following code produces the violin plots
ggplot(triadCensusDf, aes(fct_inorder(triad), nnodes)) +
  geom_violin(trim = FALSE, scale = "width") +
  stat_summary(fun = mean, geom = "point", size = 2) +
  geom_boxplot(width = 0.2, fill = "gray", outlier.shape = 4) +
  geom_point(data = triadCensusObs, col = "red", size = 2) +
  geom_line(
    data = triadCensusObs, aes(group = 1), col = "red", linewidth = 0.5
  ) +
  geom_line(
    data = percTriad, mapping = aes(group = quant),
    col = "gray", linetype = "dashed"
  ) +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  xlab("triad type") +
  ggtitle("Goodness of Fit of Triad Census Counts")
```
The p-value computed in iv) is 0, suggesting that the simulated networks are not a good fit for the observed network at time 2. 

This is further supported by the goodness of fit plots for the auxilliary statistics. The dashed grey lines in the plot define the 5% and 95% quantiles, and we can see that for 7 triad types (012, 102, 012C, 111D, 120C, 210, 300), the statistic for that triad type (red dots) lies outside this 90% range. This means that the model is a poor fit for these statistics as it is highly improbable that the observed network statistics are observed in the simulated networks.

### Task 4:

#### 1)
